import torch
torch.backends.quantized.engine = 'qnnpack'  # âœ… í•„ìˆ˜ ì¶”ê°€!

from models.gpt2 import GPT2Model
from config import GPT2Config

# 1. config ì •ì˜
config = GPT2Config()

# 2. í•™ìŠµ ì•ˆ ëœ GPT2Model ìƒì„± (ë¬´ì‘ìœ„ weight)
model = GPT2Model(config)
model.eval()

# 3. ì–‘ìí™” ì ìš©
quant_model = torch.quantization.quantize_dynamic(
    model, 
    {torch.nn.Linear}, 
    dtype=torch.qint8
)

# 4. ì„ì˜ ì…ë ¥ ìƒì„± (batch_size=1, seq_len=8)
dummy_input = torch.randint(0, config.vocab_size, (1, 8))
dummy_mask = torch.ones_like(dummy_input)

# 5. ì¶”ë¡  ì „í›„ ì¶œë ¥ ë¹„êµ
with torch.no_grad():
    out_fp32 = model(dummy_input, dummy_mask)
    out_int8 = quant_model(dummy_input, dummy_mask)

print("âœ… ì–‘ìí™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
print("ğŸ” FP32 ë§ˆì§€ë§‰ í† í° hidden:", out_fp32["last_token"][0][:5])
print("ğŸ” INT8 ë§ˆì§€ë§‰ í† í° hidden:", out_int8["last_token"][0][:5])