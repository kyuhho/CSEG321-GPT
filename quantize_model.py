# quantize_model.py

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
torch.backends.quantized.engine = 'qnnpack'  # âœ… í•„ìˆ˜ ì„¤ì •

from models.gpt2 import GPT2Model
from config import GPT2Config

torch.serialization.add_safe_globals([GPT2Config])

def load_student_model(checkpoint_path: str) -> GPT2Model:
    """Student ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    print(f"ğŸ“¦ Loading student model from {checkpoint_path}")
    ckpt = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

    config = ckpt["config"]  # âœ… ì´ë¯¸ GPT2Config ê°ì²´
    model = GPT2Model(config)
    model.load_state_dict(ckpt["state_dict"])
    model.eval()
    return model, config

def quantize_model(model: torch.nn.Module) -> torch.nn.Module:
    """ëª¨ë¸ì„ quantizeí•˜ëŠ” í•¨ìˆ˜"""
    print("âš™ï¸ Applying dynamic quantization...")
    return torch.quantization.quantize_dynamic(
        model,
        {torch.nn.Linear},
        dtype=torch.qint8
    )

def save_quantized_model(model: torch.nn.Module, config, path: str):
    """Quantized ëª¨ë¸ê³¼ configë¥¼ í•¨ê»˜ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    print(f"ğŸ’¾ Saving quantized model to {path}")
    torch.save({
        'model': model,  # ì „ì²´ ëª¨ë¸ ê°ì²´ ì €ì¥
        'config': config,
        'model_type': 'quantized'
    }, path, pickle_protocol=4)

def load_quantized_model(checkpoint_path: str):
    """Quantized ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    print(f"ğŸ“¦ Loading quantized model from {checkpoint_path}")
    ckpt = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    
    model = ckpt['model']
    config = ckpt['config']
    
    model.eval()
    return model, config

if __name__ == "__main__":
    student_ckpt_path = "saved_models/student.pt"  # âœ… ì‹¤ì œ ì €ì¥ëœ ê²½ë¡œë¡œ ìˆ˜ì •
    quant_ckpt_path = "saved_models/student_quant.pt"

    print("ğŸ“¦ Loading student model...")
    model, config = load_student_model(student_ckpt_path)

    print("âš™ï¸ Applying quantization...")
    quant_model = quantize_model(model)

    print("ğŸ’¾ Saving quantized model...")
    save_quantized_model(quant_model, config, quant_ckpt_path)

    print(f"âœ… Quantized model saved to {quant_ckpt_path}")
    
    # í…ŒìŠ¤íŠ¸: ì €ì¥ëœ ëª¨ë¸ ë‹¤ì‹œ ë¡œë“œí•´ë³´ê¸°
    print("\nğŸ§ª Testing model loading...")
    test_model, test_config = load_quantized_model(quant_ckpt_path)
    print(f"âœ… Successfully loaded quantized model with config:")
    print(f"  - Hidden size: {test_config.hidden_size}")
    print(f"  - Num layers: {test_config.num_hidden_layers}")
    print(f"  - Num attention heads: {test_config.num_attention_heads}")