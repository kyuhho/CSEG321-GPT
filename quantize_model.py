# quantize_model.py

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
import torch.nn as nn
torch.backends.quantized.engine = 'qnnpack'  # í•„ìˆ˜ ì„¤ì •

from models.gpt2 import GPT2Model as StudentGPT2Model
from config import GPT2Config

torch.serialization.add_safe_globals([GPT2Config])

def load_student_model(ckpt_path, device='cpu'):
    """ì €ì¥ëœ í•™ìƒ ëª¨ë¸ì„ ë¡œë“œ"""
    print(f"Loading student model from {ckpt_path}")
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
    
    # config ë³µì›
    config = checkpoint['config']
    
    # ëª¨ë¸ ìƒì„±
    model = StudentGPT2Model(config)
    
    # state_dict ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'])
    
    return model

def quantize_model(model):
    """ëª¨ë¸ì„ ì–‘ìí™”"""
    # ì–‘ìí™” ì¤€ë¹„
    model.eval()
    
    # ë™ì  ì–‘ìí™” ì ìš©
    quantized_model = torch.quantization.quantize_dynamic(
        model, 
        {nn.Linear}, 
        dtype=torch.qint8
    )
    
    return quantized_model

def save_quantized_model(model, config, path):
    """ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥"""
    torch.save({
        'quantized_model': model,  # ì–‘ìí™”ëœ ëª¨ë¸ ì „ì²´ ì €ì¥
        'config': config
    }, path)

def load_quantized_model(path, device='cpu'):
    """ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ"""
    print(f"Loading quantized model from {path}")
    checkpoint = torch.load(path, map_location=device)
    
    quantized_model = checkpoint['quantized_model']
    config = checkpoint['config']
    
    return quantized_model, config

if __name__ == "__main__":
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    student_ckpt_path = "saved_models/student_model.pt"  # ìœ„ì—ì„œ ì €ì¥í•œ ê²½ë¡œ
    quant_ckpt_path = "saved_models/student_quant.pt"

    print("ğŸ“¦ Loading student model...")
    model = load_student_model(student_ckpt_path, device)
    
    print("âš™ï¸  Applying quantization...")
    quant_model = quantize_model(model)
    
    # configë„ í•¨ê»˜ ì €ì¥
    checkpoint = torch.load(student_ckpt_path, map_location='cpu')
    config = checkpoint['config']
    
    print("ğŸ’¾ Saving quantized model...")
    save_quantized_model(quant_model, config, quant_ckpt_path)
    
    print(f"âœ… Quantized model saved to {quant_ckpt_path}")