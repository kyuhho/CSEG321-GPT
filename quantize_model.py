# quantize_model.py

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
torch.backends.quantized.engine = 'qnnpack'  # âœ… í•„ìˆ˜ ì„¤ì •

from models.gpt2 import GPT2Model
from config import GPT2Config

# quantize_model.py ìˆ˜ì •
def load_student_model(checkpoint_path: str) -> GPT2Model:
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    config_dict = ckpt["config"]
    config = GPT2Config(**config_dict)        # ë™ì  config ë¡œë“œ
    model = GPT2Model(config)                 # êµ¬ì¡°ë¥¼ ë§ì¶° ì´ˆê¸°í™”
    model.load_state_dict(ckpt["state_dict"]) # ì •ìƒ ë¡œë”©
    model.eval()
    return model

def quantize_model(model: torch.nn.Module) -> torch.nn.Module:
    return torch.quantization.quantize_dynamic(
        model,
        {torch.nn.Linear},
        dtype=torch.qint8
    )

def save_model(model: torch.nn.Module, path: str):
    torch.save(model.state_dict(), path)  # ë˜ëŠ” torch.save(model, path) ë¡œ ì „ì²´ ì €ì¥ ê°€ëŠ¥

if __name__ == "__main__":
    student_ckpt_path = "models/student.pt"
    quant_ckpt_path = "student_quant.pt"

    print("ğŸ“¦ Loading student model...")
    model = load_student_model(student_ckpt_path)

    print("âš™ï¸  Applying quantization...")
    quant_model = quantize_model(model)

    print("ğŸ’¾ Saving quantized model...")
    save_model(quant_model, quant_ckpt_path)

    print(f"âœ… Quantized model saved to {quant_ckpt_path}")