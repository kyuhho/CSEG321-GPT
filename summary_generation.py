# python summary_generation.py --model_type baseline ë¡œ base line ì„±ëŠ¥í…ŒìŠ¤íŠ¸
# python summary_generation.py --model_type ours ë¡œ ê²½ëŸ‰í™” í•œ ëª¨ë¸ ì„±ëŠ¥í…ŒìŠ¤íŠ¸


import torch
import argparse
import json
from tqdm import tqdm
from datasets import load_dataset
from evaluate import load as load_metric
from transformers import (
    GPT2Tokenizer,
    GPT2LMHeadModel
)
import psutil
import os

from config import GPT2Config  # í•„ìš” ì‹œ ì‚¬ìš©

def load_model(model_type, device):
    if model_type == "baseline":
        print("\U0001F4E6 Loading baseline model: gavin124/gpt2-finetuned-cnn-summarization-v2")
        tokenizer = GPT2Tokenizer.from_pretrained("gavin124/gpt2-finetuned-cnn-summarization-v2")
        tokenizer.pad_token = tokenizer.eos_token
        model = GPT2LMHeadModel.from_pretrained("gavin124/gpt2-finetuned-cnn-summarization-v2").to(device)
        return model, tokenizer

    elif model_type == "ours":
        print("\U0001F4E6 Loading our custom GPT2 model")
        from models.gpt2 import GPT2ModelForGeneration  # ë„ˆì˜ ëª¨ë¸ ì´ë¦„ì— ë§ê²Œ ìˆ˜ì •
        tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
        tokenizer.pad_token = tokenizer.eos_token
        model = GPT2ModelForGeneration.from_pretrained("path_to_your_model").to(device)
        return model, tokenizer

    else:
        raise ValueError("Unknown model type. Choose from ['baseline', 'ours'].")

def generate_summary(model, tokenizer, article, device, model_type):
    if model_type == "baseline":
        # GPT2 ëª¨ë¸ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì‚¬ìš©
        inputs = tokenizer(
            "Article: " + article.strip() + "\nSummary:",
            return_tensors="pt",
            truncation=True, padding="max_length",
            max_length=1024
        ).to(device)

        with torch.no_grad():
            outputs = model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_new_tokens=128,
                pad_token_id=tokenizer.eos_token_id,
                do_sample=True,
                temperature=0.7,
                top_p=0.9
            )

    else:  # ours
        inputs = tokenizer(
            "Article: " + article.strip() + "\nSummary:",
            return_tensors="pt",
            truncation=True, padding="max_length",
            max_length=1024
        ).to(device)

        with torch.no_grad():
            outputs = model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_new_tokens=128,
                pad_token_id=tokenizer.eos_token_id
            )

    # ì…ë ¥ ë¶€ë¶„ì„ ì œê±°í•˜ê³  ìƒˆë¡œ ìƒì„±ëœ ë¶€ë¶„ë§Œ ë°˜í™˜
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # "Summary:" ì´í›„ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    if "Summary:" in generated_text:
        summary = generated_text.split("Summary:")[-1].strip()
    else:
        summary = generated_text.strip()
    
    return summary

def main(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nğŸš€ Evaluating model: {args.model_type}\n")

    # 1. ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model(args.model_type, device)

    # 2. CNN/DailyMail ë°ì´í„° ë¡œë“œ
    dataset = load_dataset("abisee/cnn_dailymail", "3.0.0")["test"]
    dataset = dataset.select(range(args.num_samples))  # ì¼ë¶€ ìƒ˜í”Œë§Œ ì‚¬ìš©

    # 3. ìš”ì•½ ìƒì„±
    predictions = []
    references = []
    summaries_to_save = []
    memory_usages = []

    for item in tqdm(dataset, desc="ğŸ“ Generating summaries"):
        article = item["article"]
        reference = item["highlights"]
        summary = generate_summary(model, tokenizer, article, device, args.model_type)


        # âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (CPU ê¸°ì¤€)
        process = psutil.Process(os.getpid())
        mem_used = process.memory_info().rss / (1024 ** 2)  # MB ë‹¨ìœ„
        memory_usages.append(mem_used)

        predictions.append(summary)
        references.append(reference)
        summaries_to_save.append({
            "article": article[:300] + "...",
            "reference": reference,
            "summary": summary
        })

    # 4. ROUGE í‰ê°€
    rouge = load_metric("rouge")
    scores = rouge.compute(predictions=predictions, references=references, use_stemmer=True)

    print("\nğŸ“Š ROUGE Scores:")
    for key in ["rouge1", "rouge2", "rougeL", "rougeLsum"]:
        if key in scores:
            print(f"{key.upper()} - F1: {scores[key]:.4f}")
    #rouge_l = scores["rougeL"].mid.fmeasure if "rougeL" in scores else 0.0
    rouge_l = scores["rougeL"] if "rougeL" in scores else 0.0

    # í•„ìš” ì‹œ ìš”ì•½ ì €ì¥
    # # 5. ìš”ì•½ ì €ì¥
    # output_path = f"generated_summaries_{args.model_type}.json"
    # with open(output_path, "w", encoding="utf-8") as f:
    #     json.dump(summaries_to_save, f, ensure_ascii=False, indent=2)
    # print(f"\nğŸ“„ Summaries saved to {output_path}")

    # 5. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì €ì¥
    avg_memory_usage = sum(memory_usages) / len(memory_usages)

    # ğŸ”½ í‰ê°€ ê²°ê³¼ ì €ì¥
    eval_result = {
        "model_name": args.model_type,
        "rouge_l": round(rouge_l, 4),
        "memory_usage_mb": round(avg_memory_usage, 2)
    }

    with open(f"evaluation_result_{args.model_type}.json", "w") as f:
        json.dump(eval_result, f, indent=2)

    print(f"\nâœ… Evaluation result saved to evaluation_result_{args.model_type}.json")
    

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_type", choices=["baseline", "ours"], required=True)
    parser.add_argument("--num_samples", type=int, default=100, help="Number of test samples to evaluate")
    args = parser.parse_args()

    main(args)