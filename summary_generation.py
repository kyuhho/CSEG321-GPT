# python summary_generation.py --model_type baseline ë¡œ base line ì„±ëŠ¥í…ŒìŠ¤íŠ¸
# python summary_generation.py --model_type ours ë¡œ ê²½ëŸ‰í™” í•œ ëª¨ë¸ ì„±ëŠ¥í…ŒìŠ¤íŠ¸

import torch
import argparse
import json
from tqdm import tqdm
from datasets import load_dataset
from evaluate import load as load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    GPT2Tokenizer
)

from config import GPT2Config  # í•„ìš” ì‹œ ì‚¬ìš©

def load_model(model_type, device):
    if model_type == "baseline":
        print("\U0001F4E6 Loading baseline model: facebook/bart-large-cnn")
        tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")
        model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-large-cnn").to(device)
        return model, tokenizer

    elif model_type == "ours":
        print("\U0001F4E6 Loading our custom GPT2 model")
        from models.gpt2 import GPT2ModelForGeneration  # ë„ˆì˜ ëª¨ë¸ ì´ë¦„ì— ë§ê²Œ ìˆ˜ì •
        tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
        tokenizer.pad_token = tokenizer.eos_token
        model = GPT2ModelForGeneration.from_pretrained("path_to_your_model").to(device)
        return model, tokenizer

    else:
        raise ValueError("Unknown model type. Choose from ['baseline', 'ours'].")

def generate_summary(model, tokenizer, article, device, model_type):
    if model_type == "baseline":
        inputs = tokenizer(
            article, return_tensors="pt",
            truncation=True, padding=True,
            max_length=1024
        ).to(device)

        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=128)

    else:  # ours
        inputs = tokenizer(
            "Article: " + article.strip() + "\nSummary:",
            return_tensors="pt",
            truncation=True, padding="max_length",
            max_length=1024
        ).to(device)

        with torch.no_grad():
            outputs = model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_new_tokens=128,
                pad_token_id=tokenizer.eos_token_id
            )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def main(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nğŸš€ Evaluating model: {args.model_type}\n")

    # 1. ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model(args.model_type, device)

    # 2. CNN/DailyMail ë°ì´í„° ë¡œë“œ
    dataset = load_dataset("abisee/cnn_dailymail", "3.0.0")["test"]
    dataset = dataset.select(range(args.num_samples))  # ì¼ë¶€ ìƒ˜í”Œë§Œ ì‚¬ìš©

    # 3. ìš”ì•½ ìƒì„±
    predictions = []
    references = []
    summaries_to_save = []

    for item in tqdm(dataset, desc="ğŸ“ Generating summaries"):
        article = item["article"]
        reference = item["highlights"]
        summary = generate_summary(model, tokenizer, article, device, args.model_type)

        predictions.append(summary)
        references.append(reference)
        summaries_to_save.append({
            "article": article[:300] + "...",
            "reference": reference,
            "summary": summary
        })

    # 4. ROUGE í‰ê°€
    rouge = load_metric("rouge")
    scores = rouge.compute(predictions=predictions, references=references, use_stemmer=True)

    print("\nğŸ“Š ROUGE Scores:")
    for key in ["rouge1", "rouge2", "rougeL", "rougeLsum"]:
        if key in scores:
            print(f"{key.upper()} - F1: {scores[key]:.4f}")

    # 5. ìš”ì•½ ì €ì¥
    output_path = f"generated_summaries_{args.model_type}.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(summaries_to_save, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ“„ Summaries saved to {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_type", choices=["baseline", "ours"], required=True)
    parser.add_argument("--num_samples", type=int, default=100, help="Number of test samples to evaluate")
    args = parser.parse_args()

    main(args)
