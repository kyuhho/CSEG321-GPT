import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from datasets import load_dataset
from tqdm import tqdm
import evaluate  # evaluate ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë³€ê²½
import os
import sys

# ëª¨ë¸ ê²½ë¡œ ì¶”ê°€
sys.path.append('distillation')
sys.path.append('.')

def load_quantized_model(checkpoint_path: str, device):
    """Quantized ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    from models.gpt2 import GPT2Model
    from config import GPT2Config
    
    # Quantized ëª¨ë¸ ë¡œë“œ
    print(f"ğŸ“¦ Loading quantized model from {checkpoint_path}")
    
    # ì›ë³¸ student ëª¨ë¸ êµ¬ì¡°ë¥¼ ë¨¼ì € ë¡œë“œ
    student_path = "saved_models/student.pt"
    student_ckpt = torch.load(student_path, map_location='cpu', weights_only=False)
    config = student_ckpt["config"]
    
    # Student ëª¨ë¸ ìƒì„±
    model = GPT2Model(config)
    
    # Quantized ìƒíƒœ ë¡œë“œ
    quantized_state = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    model.load_state_dict(quantized_state)
    
    # Dynamic quantization ì ìš©
    model = torch.quantization.quantize_dynamic(
        model,
        {torch.nn.Linear},
        dtype=torch.qint8
    )
    
    model = model.to(device)
    model.eval()
    
    return model, config

def load_model(model_type, device):
    if model_type == "baseline":
        print("ğŸ“¦ Loading baseline model: gavin124/gpt2-finetuned-cnn-summarization-v2")
        tokenizer = GPT2Tokenizer.from_pretrained("gavin124/gpt2-finetuned-cnn-summarization-v2")
        # íŒ¨ë”© í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        model = GPT2LMHeadModel.from_pretrained("gavin124/gpt2-finetuned-cnn-summarization-v2").to(device)
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± í™•ì¸
        print(f"Model vocab size: {model.config.vocab_size}")
        print(f"Tokenizer vocab size: {len(tokenizer)}")
        
        return model, tokenizer

    elif model_type == "ours":
        print("ğŸ“¦ Loading our quantized GPT2 model")
        
        # TokenizerëŠ” baselineê³¼ ë™ì¼í•˜ê²Œ ì‚¬ìš©
        tokenizer = GPT2Tokenizer.from_pretrained("gavin124/gpt2-finetuned-cnn-summarization-v2")
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # Quantized ëª¨ë¸ ë¡œë“œ
        model, config = load_quantized_model("saved_models/student_quant.pt", device)
        
        print(f"âœ… Loaded quantized model with config:")
        print(f"  - Hidden size: {config.hidden_size}")
        print(f"  - Num layers: {config.num_hidden_layers}")
        print(f"  - Num attention heads: {config.num_attention_heads}")
        
        return model, tokenizer

    else:
        raise ValueError("Unknown model type. Choose from ['baseline', 'ours'].")

def generate_summary(model, tokenizer, article, device, model_type):
    # ê¸°ì‚¬ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸´ ì…ë ¥ ë°©ì§€)
    max_article_length = 800
    if len(article) > max_article_length:
        article = article[:max_article_length]
    
    prompt = f"Article: {article.strip()}\nSummary:"
    
    try:
        if model_type == "baseline":
            # íŒ¨ë”© ì—†ì´ ì²˜ë¦¬
            inputs = tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=512,  # 1024ì—ì„œ 512ë¡œ ì¤„ì„
                padding=False    # íŒ¨ë”© ì œê±°
            ).to(device)
            
            # ì…ë ¥ ê¸¸ì´ í™•ì¸
            input_length = inputs["input_ids"].shape[1]
            print(f"Input tokens: {input_length}")
            
            # ì…ë ¥ì´ ë„ˆë¬´ ê¸¸ë©´ ë” ì¤„ì´ê¸°
            if input_length > 400:
                inputs = tokenizer(
                    prompt,
                    return_tensors="pt",
                    truncation=True,
                    max_length=300,
                    padding=False
                ).to(device)

            with torch.no_grad():
                outputs = model.generate(
                    input_ids=inputs["input_ids"],
                    max_new_tokens=100,  # 128ì—ì„œ 100ìœ¼ë¡œ ì¤„ì„
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    # attention_mask ì œê±° (íŒ¨ë”©ì´ ì—†ìœ¼ë¯€ë¡œ)
                )

        else:  # ours - quantized model
            inputs = tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=False
            ).to(device)

            # ìš°ë¦¬ ëª¨ë¸ì€ custom GPT2Modelì´ë¯€ë¡œ ì§ì ‘ forward pass ìˆ˜í–‰
            with torch.no_grad():
                # ì…ë ¥ ê¸¸ì´ í™•ì¸
                input_length = inputs["input_ids"].shape[1]
                max_new_tokens = 100
                
                input_ids = inputs["input_ids"]
                generated_ids = input_ids.clone()
                
                # Auto-regressive generation
                for _ in range(max_new_tokens):
                    # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ í† í°ë“¤ë¡œ ë‹¤ìŒ í† í° ì˜ˆì¸¡
                    model_outputs = model(input_ids=generated_ids)
                    hidden_states = model_outputs['last_hidden_state']
                    
                    # Hidden statesë¥¼ logitsë¡œ ë³€í™˜
                    logits = model.hidden_state_to_token(hidden_states)
                    
                    # ë§ˆì§€ë§‰ í† í°ì˜ logitsë§Œ ì‚¬ìš©
                    next_token_logits = logits[:, -1, :]
                    
                    # ë‹¤ìŒ í† í° ì„ íƒ (greedy decoding)
                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                    
                    # EOS í† í°ì´ë©´ ìƒì„± ì¤‘ë‹¨
                    if next_token.item() == tokenizer.eos_token_id:
                        break
                    
                    # ìƒì„±ëœ í† í°ì„ ì‹œí€€ìŠ¤ì— ì¶”ê°€
                    generated_ids = torch.cat([generated_ids, next_token], dim=-1)
                
                outputs = generated_ids

        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # "Summary:" ì´í›„ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "Summary:" in generated_text:
            summary = generated_text.split("Summary:")[-1].strip()
        else:
            # ì…ë ¥ ë¶€ë¶„ ì œê±°
            input_text = tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True)
            if input_text in generated_text:
                summary = generated_text.replace(input_text, "").strip()
            else:
                summary = generated_text.strip()
        
        return summary
        
    except Exception as e:
        print(f"Error generating summary: {e}")
        import traceback
        traceback.print_exc()
        return "Error: Could not generate summary"

def main(args):
    # CUDA ë””ë²„ê¹… í™œì„±í™”
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nğŸš€ Evaluating model: {args.model_type}")
    print(f"ğŸ”§ Using device: {device}\n")

    try:
        # 1. ëª¨ë¸ ë¡œë“œ
        model, tokenizer = load_model(args.model_type, device)
        
        # 2. CNN/DailyMail ë°ì´í„° ë¡œë“œ
        print("ğŸ“Š Loading dataset...")
        dataset = load_dataset("abisee/cnn_dailymail", "3.0.0")["test"]
        dataset = dataset.select(range(args.num_samples))

        # 3. ìš”ì•½ ìƒì„±
        predictions = []
        references = []
        summaries_to_save = []
        
        successful_generations = 0
        
        for i, item in enumerate(tqdm(dataset, desc="ğŸ“ Generating summaries")):
            try:
                article = item["article"]
                reference = item["highlights"]
                summary = generate_summary(model, tokenizer, article, device, args.model_type)
                
                if summary and not summary.startswith("Error:"):
                    predictions.append(summary)
                    references.append(reference)
                    summaries_to_save.append({
                        "id": i,
                        "article": article[:300] + "...",
                        "reference": reference,
                        "summary": summary
                    })
                    successful_generations += 1
                else:
                    print(f"Failed to generate summary for sample {i}")
                    
            except Exception as e:
                print(f"Error processing sample {i}: {e}")
                continue

        print(f"\nâœ… Successfully generated {successful_generations}/{args.num_samples} summaries")

        if len(predictions) > 0:
            # 4. ROUGE í‰ê°€
            print("ğŸ“Š Computing ROUGE scores...")
            rouge = evaluate.load("rouge")  # load_metric ëŒ€ì‹  evaluate.load ì‚¬ìš©
            scores = rouge.compute(predictions=predictions, references=references, use_stemmer=True)

            print("\nğŸ“Š ROUGE Scores:")
            for key in ["rouge1", "rouge2", "rougeL", "rougeLsum"]:
                if key in scores:
                    print(f"{key.upper()} - F1: {scores[key]:.4f}")
                    
            # ê²°ê³¼ ì €ì¥
            import json
            output_file = f"summaries_{args.model_type}_{args.num_samples}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "scores": scores,
                    "summaries": summaries_to_save
                }, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ Results saved to {output_file}")
            
        else:
            print("âŒ No successful summaries generated!")

    except Exception as e:
        print(f"âŒ Main execution error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_type", type=str, required=True, choices=["baseline", "ours"])
    parser.add_argument("--num_samples", type=int, default=50)
    args = parser.parse_args()
    main(args)